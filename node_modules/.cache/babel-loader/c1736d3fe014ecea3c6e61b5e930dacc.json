{"ast":null,"code":"function tokenize(text, tokens) {\n  const compiledRegex = new RegExp(Object.entries(tokens).map(([type, regex]) => `(?<${type}>${regex.source})`).join('|'), 'yi');\n  let index = 0;\n  const ast = [];\n\n  while (index < text.length) {\n    compiledRegex.lastIndex = index;\n    const result = text.match(compiledRegex);\n\n    if (result !== null) {\n      const [type, text] = Object.entries(result.groups).find(([name, group]) => group !== undefined);\n      index += text.length;\n\n      if (!type.startsWith('_')) {\n        ast.push({\n          type,\n          text\n        });\n      }\n    } else {\n      throw new Error(`No matching tokenizer rule found at: [${text.substring(index)}]`);\n    }\n  }\n\n  return ast;\n}\n\nmodule.exports = {\n  tokenize\n};","map":{"version":3,"sources":["/home/rosiane/back/node_modules/knex/lib/dialects/sqlite3/schema/internal/tokenizer.js"],"names":["tokenize","text","tokens","compiledRegex","RegExp","Object","entries","map","type","regex","source","join","index","ast","length","lastIndex","result","match","groups","find","name","group","undefined","startsWith","push","Error","substring","module","exports"],"mappings":"AAAA,SAASA,QAAT,CAAkBC,IAAlB,EAAwBC,MAAxB,EAAgC;AAC9B,QAAMC,aAAa,GAAG,IAAIC,MAAJ,CACpBC,MAAM,CAACC,OAAP,CAAeJ,MAAf,EACGK,GADH,CACO,CAAC,CAACC,IAAD,EAAOC,KAAP,CAAD,KAAoB,MAAKD,IAAK,IAAGC,KAAK,CAACC,MAAO,GADrD,EAEGC,IAFH,CAEQ,GAFR,CADoB,EAIpB,IAJoB,CAAtB;AAOA,MAAIC,KAAK,GAAG,CAAZ;AACA,QAAMC,GAAG,GAAG,EAAZ;;AAEA,SAAOD,KAAK,GAAGX,IAAI,CAACa,MAApB,EAA4B;AAC1BX,IAAAA,aAAa,CAACY,SAAd,GAA0BH,KAA1B;AACA,UAAMI,MAAM,GAAGf,IAAI,CAACgB,KAAL,CAAWd,aAAX,CAAf;;AAEA,QAAIa,MAAM,KAAK,IAAf,EAAqB;AACnB,YAAM,CAACR,IAAD,EAAOP,IAAP,IAAeI,MAAM,CAACC,OAAP,CAAeU,MAAM,CAACE,MAAtB,EAA8BC,IAA9B,CACnB,CAAC,CAACC,IAAD,EAAOC,KAAP,CAAD,KAAmBA,KAAK,KAAKC,SADV,CAArB;AAIAV,MAAAA,KAAK,IAAIX,IAAI,CAACa,MAAd;;AAEA,UAAI,CAACN,IAAI,CAACe,UAAL,CAAgB,GAAhB,CAAL,EAA2B;AACzBV,QAAAA,GAAG,CAACW,IAAJ,CAAS;AAAEhB,UAAAA,IAAF;AAAQP,UAAAA;AAAR,SAAT;AACD;AACF,KAVD,MAUO;AACL,YAAM,IAAIwB,KAAJ,CACH,yCAAwCxB,IAAI,CAACyB,SAAL,CAAed,KAAf,CAAsB,GAD3D,CAAN;AAGD;AACF;;AAED,SAAOC,GAAP;AACD;;AAEDc,MAAM,CAACC,OAAP,GAAiB;AACf5B,EAAAA;AADe,CAAjB","sourcesContent":["function tokenize(text, tokens) {\n  const compiledRegex = new RegExp(\n    Object.entries(tokens)\n      .map(([type, regex]) => `(?<${type}>${regex.source})`)\n      .join('|'),\n    'yi'\n  );\n\n  let index = 0;\n  const ast = [];\n\n  while (index < text.length) {\n    compiledRegex.lastIndex = index;\n    const result = text.match(compiledRegex);\n\n    if (result !== null) {\n      const [type, text] = Object.entries(result.groups).find(\n        ([name, group]) => group !== undefined\n      );\n\n      index += text.length;\n\n      if (!type.startsWith('_')) {\n        ast.push({ type, text });\n      }\n    } else {\n      throw new Error(\n        `No matching tokenizer rule found at: [${text.substring(index)}]`\n      );\n    }\n  }\n\n  return ast;\n}\n\nmodule.exports = {\n  tokenize,\n};\n"]},"metadata":{},"sourceType":"script"}